{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Ensemble/consensus approach\n",
    "This notebook explores the use of an 'ensemble' or 'consensus' approach where two LLMs are used to see what the agreement rate between them is.\n",
    "\n",
    "Non-agreeing records are filtered out to see what the performance is like on only those that agree on all metrics across the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.helpers import load_config_from_yaml\n",
    "from src.text_preprocessor import TextPreprocessor\n",
    "from src.prompt_builder import PromptBuilder\n",
    "from src.model_request import ModelRequest\n",
    "from src.extractor_pipeline import ExtractorPipeline\n",
    "from src.post_processor import PostProcessor\n",
    "from src.custom_logging import setup_logging\n",
    "from src.cme_evaluator import CMEEvaluator\n",
    "from src.load_data import load_dataframe_from_s3, save_dataframe_to_s3\n",
    "import config.pipeline_config as conf\n",
    "from config.validation_config import ValidSchema, MultiSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_file_path = \"./config/local.yaml\"\n",
    "yaml_conf = load_config_from_yaml(file_path=conf_file_path)\n",
    "\n",
    "bucket_name = yaml_conf.get(\"BUCKET_NAME\")\n",
    "id_col = yaml_conf.get(\"ID_COL\")\n",
    "data_name = yaml_conf.get(\"THE_DATA\")\n",
    "\n",
    "model_id = yaml_conf.get(\"MODEL_ID_1\")\n",
    "model_args = yaml_conf.get(\"MODEL_ARGS_1\")\n",
    "output_folder = yaml_conf.get(\"YOUR_S3_FOLDER\")\n",
    "cme_prompt_id = yaml_conf.get(\"GPT_PROMPT_MANAGEMENT_ID\")\n",
    "cme_prompt_name = yaml_conf.get(\"GPT_PROMPT_MANAGEMENT_NAME\")\n",
    "\n",
    "cme_multi_prompt_id = yaml_conf.get(\"MULTI_TUMOUR_PROMPT_MANAGEMENT_ID\")\n",
    "cme_multi_prompt_name = yaml_conf.get(\"MULTI_TUMOUR_PROMPT_MANAGEMENT_NAME\")\n",
    "\n",
    "# Load in the records\n",
    "float_columns = [f\"ER_SCORE_{i+1}\" for i in range(4)] + [f\"PR_SCORE_{i+1}\" for i in range(4)]\n",
    "records = load_dataframe_from_s3(bucket_name, data_name, float_columns)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(f\"Loaded {records.shape[0]} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processor = PostProcessor(records, conf.multi_tumour_accepted_values.keys(), conf.multi_tumour_accepted_values)\n",
    "records[\"Multiple Tumours\"] = records[\"Multiple Tumours\"].fillna(\"0\")\n",
    "records = records.apply(post_processor.apply_general_mapping, mapping = {\"y\":\"1\"}, cols_to_map = [\"Multiple Tumours\"], axis = 1)\n",
    "\n",
    "records[\"Multiple Tumours New\"] = records[\"Multiple Tumours New\"].fillna(\"0\")\n",
    "records = records.apply(post_processor.apply_general_mapping, mapping = {\"y\":\"1\"}, cols_to_map = [\"Multiple Tumours New\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Flag Multiple Tumours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_logging(enable_console=False,\n",
    "              enable_file=True,\n",
    "              console_log_level=conf.console_log_level,\n",
    "              log_dir=conf.log_dir)\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "prompter = PromptBuilder(model_id = model_id,\n",
    "                         system_prompt = conf.multi_tumour_system_prompt,\n",
    "                         prompt_layout = conf.multi_tumour_prompt)\n",
    "\n",
    "requester = ModelRequest(model_id,\n",
    "                         model_args,\n",
    "                         prompter)\n",
    "\n",
    "extractor_pipeline = ExtractorPipeline(config_file_path=conf_file_path,\n",
    "                                       preprocessor=preprocessor,\n",
    "                                       model_request=requester,\n",
    "                                       valid_structure=MultiSchema,\n",
    "                                       accepted_values = conf.multi_tumour_accepted_values)\n",
    "\n",
    "multi_tumour_output_df = extractor_pipeline.run(df=records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_tumour_reports = multi_tumour_output_df[multi_tumour_output_df['multi_tumour']==\"1\"]\n",
    "single_tumour_reports = multi_tumour_output_df[multi_tumour_output_df['multi_tumour']==\"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(multi_tumour_reports)} reports were flagged as containing multiple tumours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_bars(ax, fmt=\"{:.1f}%\"):\n",
    "    \"\"\"Annotate each bar in a barplot.\"\"\"\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        if height > 0:\n",
    "            ax.text(\n",
    "                x=p.get_x() + p.get_width() / 2,\n",
    "                y=height+1,\n",
    "                s=fmt.format(height),\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                color='black',\n",
    "                fontsize=10,\n",
    "                fontweight='bold'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "prompter = PromptBuilder(model_id = model_id,\n",
    "                         system_prompt = conf.extraction_system_prompt,\n",
    "                         prompt_id = cme_prompt_id,\n",
    "                         prompt_version = conf.extraction_prompt_version,\n",
    "                         accepted_values = conf.accepted_values)\n",
    "\n",
    "requester = ModelRequest(model_id,\n",
    "                         model_args,\n",
    "                         prompter)\n",
    "\n",
    "extractor_pipeline = ExtractorPipeline(config_file_path=conf_file_path,\n",
    "                                       preprocessor=preprocessor,\n",
    "                                       model_request=requester,\n",
    "                                       valid_structure=ValidSchema,\n",
    "                                       accepted_values = conf.accepted_values)\n",
    "\n",
    "metrics_output_df_1 = extractor_pipeline.run(df=single_tumour_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processor = PostProcessor(metrics_output_df_1, conf.accepted_values.keys(), conf.accepted_values)\n",
    "\n",
    "functions = {\n",
    "    \"map_two_part_scores\": post_processor.map_two_part_scores,\n",
    "    \"map_score\": post_processor.map_score,\n",
    "    \"score_to_status\": post_processor.score_to_status,\n",
    "    \"apply_general_mapping\": post_processor.apply_general_mapping,\n",
    "}\n",
    "\n",
    "settings = {\n",
    "    \"map_two_part_scores\": {\n",
    "        \"enabled\":True,\n",
    "        \"args\": [{\"cols_to_map\":[\"er_score\", \"pr_score\"]}]\n",
    "    },\n",
    "    \"map_score\": {\n",
    "        \"enabled\": True,\n",
    "        \"args\": [{\"cols_to_map\":[\"er_score\", \"pr_score\"]}]\n",
    "    },\n",
    "    \"score_to_status\": {\n",
    "        \"enabled\": True,\n",
    "        \"args\": [{\"pairs\": [(\"er_score\",\"er_status\"),(\"pr_score\",\"pr_status\")]}]\n",
    "    },\n",
    "    \"apply_general_mapping\": {\n",
    "        \"enabled\": True,\n",
    "        \"args\": [\n",
    "            {\"mapping\":{\"0\": \"negative (0)\", \"1+\": \"negative (1+)\", \"2+\": \"borderline (2+)\", \"3+\": \"positive (3+)\"},\n",
    "                 \"cols_to_map\":[\"her2_status\"]},\n",
    "            {\"mapping\":{\"null\": np.nan},\n",
    "                 \"cols_to_map\":[\"er_status\", \"er_score\", \"pr_status\", \"pr_score\", \"her2_status\"]}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "metrics_output_df_processed_1 = post_processor.run(functions, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2\n",
    "model_id = yaml_conf.get(\"MODEL_ID_2\")\n",
    "model_args = yaml_conf.get(\"MODEL_ARGS_2\")\n",
    "output_folder = yaml_conf.get(\"YOUR_S3_FOLDER\")\n",
    "cme_prompt_id = yaml_conf.get(\"SONNET_PROMPT_MANAGEMENT_ID\")\n",
    "cme_prompt_name = yaml_conf.get(\"SONNET_PROMPT_MANAGEMENT_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "prompter = PromptBuilder(model_id = model_id,\n",
    "                         system_prompt = conf.extraction_system_prompt,\n",
    "                         prompt_id = cme_prompt_id,\n",
    "                         prompt_version = 17,\n",
    "                         accepted_values = conf.accepted_values)\n",
    "\n",
    "requester = ModelRequest(model_id,\n",
    "                         model_args,\n",
    "                         prompter)\n",
    "\n",
    "extractor_pipeline = ExtractorPipeline(config_file_path=conf_file_path,\n",
    "                                       preprocessor=preprocessor,\n",
    "                                       model_request=requester,\n",
    "                                       valid_structure=ValidSchema,\n",
    "                                       accepted_values = conf.accepted_values)\n",
    "\n",
    "metrics_output_df_2 = extractor_pipeline.run(df=single_tumour_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processor = PostProcessor(metrics_output_df_2, conf.accepted_values.keys(), conf.accepted_values)\n",
    "\n",
    "functions = {\n",
    "    \"map_two_part_scores\": post_processor.map_two_part_scores,\n",
    "    \"map_score\": post_processor.map_score,\n",
    "    \"score_to_status\": post_processor.score_to_status,\n",
    "    \"apply_general_mapping\": post_processor.apply_general_mapping,\n",
    "}\n",
    "\n",
    "settings = {\n",
    "    \"map_two_part_scores\": {\n",
    "        \"enabled\":True,\n",
    "        \"args\": [{\"cols_to_map\":[\"er_score\", \"pr_score\"]}]\n",
    "    },\n",
    "    \"map_score\": {\n",
    "        \"enabled\": True,\n",
    "        \"args\": [{\"cols_to_map\":[\"er_score\", \"pr_score\"]}]\n",
    "    },\n",
    "    \"score_to_status\": {\n",
    "        \"enabled\": True,\n",
    "        \"args\": [{\"pairs\": [(\"er_score\",\"er_status\"),(\"pr_score\",\"pr_status\")]}]\n",
    "    },\n",
    "    \"apply_general_mapping\": {\n",
    "        \"enabled\": True,\n",
    "        \"args\": [\n",
    "            {\"mapping\":{\"0\": \"negative (0)\", \"1+\": \"negative (1+)\", \"2+\": \"borderline (2+)\", \"3+\": \"positive (3+)\"},\n",
    "                 \"cols_to_map\":[\"her2_status\"]},\n",
    "            {\"mapping\":{\"null\": np.nan},\n",
    "                 \"cols_to_map\":[\"er_status\", \"er_score\", \"pr_status\", \"pr_score\", \"her2_status\"]}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "metrics_output_df_processed_2 = post_processor.run(functions, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Combine the two results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_output_df_processed_2 = metrics_output_df_processed_2.drop(['REPORT', 'preprocessed_REPORT', 'model_output'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = metrics_output_df_processed_1.merge(metrics_output_df_processed_2, on='PATHOLOGY_ID', how='inner', suffixes=('_m1','_m2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['status'] = np.select(\n",
    "    [\n",
    "        (combined[['status_m1', 'status_m2']] == 'validation_failed').any(axis=1),\n",
    "        (combined[['status_m1', 'status_m2']] == 'invalid').any(axis=1),\n",
    "        (combined[['status_m1', 'status_m2']] == 'partial').any(axis=1),\n",
    "        (combined['status_m1'].eq('valid') & combined['status_m2'].eq('valid'))\n",
    "    ],\n",
    "    [\n",
    "        'validation_failed',\n",
    "        'invalid',\n",
    "        'partial',\n",
    "        'valid'\n",
    "    ]\n",
    ")\n",
    "\n",
    "combined['status_processed'] = np.select(\n",
    "    [\n",
    "        (combined[['status_processed_m1', 'status_processed_m2']] == 'validation_failed').any(axis=1),\n",
    "        (combined[['status_processed_m1', 'status_processed_m2']] == 'invalid').any(axis=1),\n",
    "        (combined[['status_processed_m1', 'status_processed_m2']] == 'partial').any(axis=1),\n",
    "        (combined['status_processed_m1'].eq('valid') & combined['status_processed_m2'].eq('valid'))\n",
    "    ],\n",
    "    [\n",
    "        'validation_failed',\n",
    "        'invalid',\n",
    "        'partial',\n",
    "        'valid'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_compare_cols = {\"er_status_p_m1\": \"er_status_p_m2\",\n",
    "                         \"er_score_p_m1\": \"er_score_p_m2\",\n",
    "                         \"pr_status_p_m1\": \"pr_status_p_m2\",\n",
    "                         \"pr_score_p_m1\": \"pr_score_p_m2\",\n",
    "                         \"her2_status_p_m1\": \"her2_status_p_m2\"}\n",
    "status_column = 'status_processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_values = [\"positive\", \"negative\", \"not performed\"]\n",
    "her2_status_values = [\"negative (unknown)\", \"negative (0)\", \"negative (1+)\", \"borderline (2+)\", \"positive (3+)\", \"not performed\"]\n",
    "score_values = [\"0\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\"]\n",
    "\n",
    "accepted_values = {\n",
    "    \"er_status_p_m2\": status_values,\n",
    "    \"er_score_p_m2\": score_values,\n",
    "    \"pr_status_p_m2\": status_values,\n",
    "    \"pr_score_p_m2\": score_values,\n",
    "    \"her2_status_p_m2\": her2_status_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = CMEEvaluator(comparison_dict=metric_compare_cols,\n",
    "                         accepted_values=accepted_values,\n",
    "                         id_col=id_col,\n",
    "                         df=combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_cols_1 = ['ER_SCORE_1','PR_SCORE_1']\n",
    "results_df = evaluator.df.copy()\n",
    "ground_truth_data = load_dataframe_from_s3(bucket_name,data_name, float_cols_1)\n",
    "\n",
    "m1_cols = ['er_status_p_m1','er_score_p_m1','pr_status_p_m1','pr_score_p_m1','her2_status_p_m1']\n",
    "m2_cols = ['er_status_p_m2','er_score_p_m2','pr_status_p_m2','pr_score_p_m2','her2_status_p_m2']\n",
    "combined[m1_cols+m2_cols] = combined[m1_cols+m2_cols].fillna('blank')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Compare agreement rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Comparing {len(combined)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-document number of correct predictions\n",
    "combined[\"num_correct\"] = (combined[m1_cols].values == combined[m2_cols].values).sum(axis=1)\n",
    "\n",
    "correct_dist = combined[\"num_correct\"].value_counts(normalize=True).reindex([0,1,2,3,4,5]).sort_index() * 100\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax1 = sns.barplot(x=['0','1','2','3','4','5'], y=correct_dist.values)\n",
    "plt.xlabel(\"Number of agreeing entities\")\n",
    "plt.ylabel(\"Percentage of documents (%)\")\n",
    "plt.title(\"Distribution of Documents by Number of Agreeing Entities\")\n",
    "plt.ylim(0, 110)\n",
    "annotate_bars(ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy per metric\n",
    "metric_accuracy = {}\n",
    "\n",
    "for ent, pred in zip(m1_cols, m2_cols):\n",
    "    metric_accuracy[ent] = (combined[ent] == combined[pred]).mean() * 100\n",
    "\n",
    "acc_series = pd.Series(metric_accuracy)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax1 = sns.barplot(x=acc_series.index, y=acc_series.values)\n",
    "plt.ylabel(\"Agreement (%)\")\n",
    "plt.xlabel(\"Extracted Metrics\")\n",
    "plt.title(\"Agreement per Entity\")\n",
    "plt.ylim(0, 110)\n",
    "annotate_bars(ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for each metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "for col1, col2 in zip(m1_cols, m2_cols):\n",
    "    if col1 in ['er_status_p_m1','pr_status_p_m1']:\n",
    "        labels = ['positive','negative','not performed','blank']\n",
    "    elif col1 in ['er_score_p_m1','pr_score_p_m1']:\n",
    "        labels = ['0','2','3','4','5','6','7','8','blank']\n",
    "    else:\n",
    "        labels = ['negative (unknown)','negative (0)','negative (1+)','borderline (2+)','positive (3+)','not performed','blank']\n",
    "    cm = confusion_matrix(combined[col1], combined[col2], labels=labels)\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    ax = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=labels, yticklabels=labels, linewidths=0.1, linecolor='grey',)\n",
    "    plt.xlabel(\"Model 1\")\n",
    "    plt.ylabel(\"Model 2\")\n",
    "    plt.title(f\"Confusion Matrix for {col1}\")\n",
    "\n",
    "    for text in ax.texts:\n",
    "        if text.get_text() == \"0\":\n",
    "            text.set_text(\"\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Performance of agreed records\n",
    "Only those records where both models agreed on the values for all metrics are used for this performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_agree = combined[combined['num_correct'] == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(all_agree)} records where all metric values agreed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_compare_cols = {\"ER_STATUS_1\": \"er_status_p_m2\",\n",
    "                         \"ER_SCORE_1\": \"er_score_p_m2\",\n",
    "                         \"PR_STATUS_1\": \"pr_status_p_m2\",\n",
    "                         \"PR_SCORE_1\": \"pr_score_p_m2\",\n",
    "                         \"HER2_STATUS_1\": \"her2_status_p_m2\"}\n",
    "status_column = 'status_processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_cols_1 = ['ER_SCORE_1','PR_SCORE_1']\n",
    "results_df = all_agree.copy()\n",
    "ground_truth_data = load_dataframe_from_s3(bucket_name,data_name, float_cols_1)\n",
    "\n",
    "actual_cols = ['ER_STATUS_1','ER_SCORE_1','PR_STATUS_1','PR_SCORE_1','HER2_STATUS_1']\n",
    "extracted_cols = ['er_status_p_m1','er_score_p_m1','pr_status_p_m1','pr_score_p_m1','her2_status_p_m1']\n",
    "llm_extraction = results_df[[id_col]+extracted_cols].fillna('blank')\n",
    "ground_truth_data = ground_truth_data[[id_col]+actual_cols].fillna('blank')\n",
    "\n",
    "gt_vs_llm = ground_truth_data.merge(llm_extraction, on = id_col, how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-document number of correct predictions\n",
    "gt_vs_llm[\"num_correct\"] = (gt_vs_llm[actual_cols].values == gt_vs_llm[extracted_cols].values).sum(axis=1)\n",
    "\n",
    "correct_dist = gt_vs_llm[\"num_correct\"].value_counts(normalize=True).reindex([0,1,2,3,4,5]).sort_index() * 100\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax1 = sns.barplot(x=['0','1','2','3','4','5'], y=correct_dist.values)\n",
    "plt.xlabel(\"Number of correctly extracted entities\")\n",
    "plt.ylabel(\"Percentage of documents (%)\")\n",
    "plt.title(\"Distribution of Documents by Number of Correct Entity Extractions\")\n",
    "plt.ylim(0, 110)\n",
    "annotate_bars(ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy per metric\n",
    "metric_accuracy = {}\n",
    "\n",
    "for ent, pred in zip(actual_cols, extracted_cols):\n",
    "    metric_accuracy[ent] = (gt_vs_llm[ent] == gt_vs_llm[pred]).mean() * 100\n",
    "\n",
    "acc_series = pd.Series(metric_accuracy)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax1 = sns.barplot(x=acc_series.index, y=acc_series.values)\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xlabel(\"Extracted Metrics\")\n",
    "plt.title(\"Accuracy per Entity\")\n",
    "plt.ylim(0, 110)\n",
    "annotate_bars(ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for each metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "for ent, pred in zip(actual_cols, extracted_cols):\n",
    "    if ent in ['ER_STATUS_1','PR_STATUS_1']:\n",
    "        labels = ['positive','negative','not performed','blank']\n",
    "    elif ent in ['ER_SCORE_1','PR_SCORE_1']:\n",
    "        labels = ['0','2','3','4','5','6','7','8','blank']\n",
    "    else:\n",
    "        labels = ['negative (unknown)','negative (0)','negative (1+)','borderline (2+)','positive (3+)','not performed','blank']\n",
    "    cm = confusion_matrix(gt_vs_llm[ent], gt_vs_llm[pred], labels=labels)\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    ax = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=labels, yticklabels=labels, linewidths=0.1, linecolor='grey',)\n",
    "    plt.xlabel(\"Model 1\")\n",
    "    plt.ylabel(\"Model 2\")\n",
    "    plt.title(f\"Confusion Matrix for {ent}\")\n",
    "\n",
    "    for text in ax.texts:\n",
    "        if text.get_text() == \"0\":\n",
    "            text.set_text(\"\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eval_df = records.merge(all_agree.drop('REPORT', axis = 1), on = id_col, how = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to S3 if required\n",
    "# file_name = \"file_name_here\"\n",
    "# save_dataframe_to_s3(all_eval_df,bucket_name,f\"ensemble/{file_name}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
