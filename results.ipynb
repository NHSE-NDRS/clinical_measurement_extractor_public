{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "from src.load_data import load_dataframe_from_s3\n",
    "from src.helpers import load_config_from_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in data here\n",
    "conf_file_path = \"./config/local.yaml\"\n",
    "yaml_conf = load_config_from_yaml(file_path=conf_file_path)\n",
    "bucket_name = yaml_conf.get(\"BUCKET_NAME\")\n",
    "id_col = yaml_conf.get(\"ID_COL\")\n",
    "data_name = yaml_conf.get(\"THE_DATA\")\n",
    "human_labels = yaml_conf.get(\"HUMAN\")\n",
    "pipeline_outputs = yaml_conf.get(\"FINAL_RESULTS\")\n",
    "\n",
    "float_cols_1 = ['ER_SCORE_1','PR_SCORE_1']\n",
    "float_cols_2 = ['er_score_p','pr_score_p']\n",
    "results_df = load_dataframe_from_s3(bucket_name,pipeline_outputs, float_cols_1 + float_cols_2)\n",
    "# results_df = results_df[results_df['BATCH'] == 5]# filter on batch if required\n",
    "ground_truth_data = load_dataframe_from_s3(bucket_name,data_name, float_cols_1)\n",
    "human_extraction = load_dataframe_from_s3(bucket_name,human_labels, float_cols_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_bars(ax, fmt=\"{:.1f}%\"):\n",
    "    \"\"\"Annotate each bar in a barplot.\"\"\"\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        if height > 0:\n",
    "            ax.text(\n",
    "                x=p.get_x() + p.get_width() / 2,\n",
    "                y=height+1,\n",
    "                s=fmt.format(height),\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                color='black',\n",
    "                fontsize=10,\n",
    "                fontweight='bold'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Results for multiple tumour flagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_cols = ['Multiple Tumours', 'Multiple Tumours New']\n",
    "extracted_cols = ['multi_tumour']\n",
    "multi_results_df = results_df[[id_col]+actual_cols+extracted_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Accuracy per metric\n",
    "metric_accuracy = {}\n",
    "\n",
    "for ent, pred in zip(actual_cols, extracted_cols*2):\n",
    "    metric_accuracy[ent] = (multi_results_df[ent] == multi_results_df[pred]).mean() * 100\n",
    "\n",
    "acc_series = pd.Series(metric_accuracy)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax1 = sns.barplot(x=acc_series.index, y=acc_series.values)\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xlabel(\"Extracted Metrics\")\n",
    "plt.title(\"Accuracy per Entity\")\n",
    "plt.ylim(0, 110)\n",
    "annotate_bars(ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for each metric\n",
    "\n",
    "for ent, pred in zip(actual_cols, extracted_cols*2):\n",
    "    labels = [0,1]\n",
    "    cm = confusion_matrix(multi_results_df[ent], multi_results_df[pred], labels=labels)\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    ax = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=labels, yticklabels=labels, linewidths=0.1, linecolor='grey',)\n",
    "    plt.xlabel(\"Extracted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix for {ent}\")\n",
    "\n",
    "    for text in ax.texts:\n",
    "        if text.get_text() == \"0\":\n",
    "            text.set_text(\"\")\n",
    "    plt.show()\n",
    "\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        multi_results_df[ent],\n",
    "        multi_results_df[pred],\n",
    "        labels=labels,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    prf_table = pd.DataFrame({\n",
    "        \"Class\": labels,\n",
    "        \"Precision\": np.round(precision,4),\n",
    "        \"Recall\": np.round(recall,4),\n",
    "        \"F1 Score\": np.round(f1,4),\n",
    "        \"Support\": support\n",
    "    })\n",
    "\n",
    "    prf_table = prf_table.set_index(\"Class\")\n",
    "\n",
    "    print(f\"\\n=== Precision, Recall, F1 for {ent} ===\")\n",
    "    print(prf_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Results for metric extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "actual_cols = ['ER_STATUS_1','ER_SCORE_1','PR_STATUS_1','PR_SCORE_1','HER2_STATUS_1']\n",
    "extracted_cols = ['er_status_p','er_score_p','pr_status_p','pr_score_p','her2_status_p']\n",
    "llm_extraction = results_df[results_df['multi_tumour'] == 0]\n",
    "llm_extraction = llm_extraction[[id_col]+extracted_cols].fillna('blank')\n",
    "ground_truth_data = ground_truth_data[[id_col]+actual_cols].fillna('blank')\n",
    "human_extraction = human_extraction[[id_col]+actual_cols].fillna('blank')\n",
    "\n",
    "gt_vs_llm = ground_truth_data.merge(llm_extraction, on = id_col, how = 'inner')\n",
    "gt_vs_hum = ground_truth_data.merge(human_extraction, on = id_col, how = 'inner', suffixes = (None,'_hum'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-document number of correct predictions\n",
    "gt_vs_llm[\"num_correct\"] = (gt_vs_llm[actual_cols].values == gt_vs_llm[extracted_cols].values).sum(axis=1)\n",
    "\n",
    "# Compute percentage distribution\n",
    "correct_dist = gt_vs_llm[\"num_correct\"].value_counts(normalize=True).reindex([0,1,2,3,4,5]).sort_index() * 100\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax1 = sns.barplot(x=['0','1','2','3','4','5'], y=correct_dist.values)\n",
    "plt.xlabel(\"Number of correctly extracted entities\")\n",
    "plt.ylabel(\"Percentage of documents (%)\")\n",
    "plt.title(\"Distribution of Documents by Number of Correct Entity Extractions\")\n",
    "plt.ylim(0, 110)\n",
    "annotate_bars(ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy per metric (5 bars)\n",
    "metric_accuracy = {}\n",
    "\n",
    "for ent, pred in zip(actual_cols, extracted_cols):\n",
    "    metric_accuracy[ent] = (gt_vs_llm[ent] == gt_vs_llm[pred]).mean() * 100\n",
    "\n",
    "acc_series = pd.Series(metric_accuracy)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax1 = sns.barplot(x=acc_series.index, y=acc_series.values)\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xlabel(\"Extracted Metrics\")\n",
    "plt.title(\"Accuracy per Entity\")\n",
    "plt.ylim(0, 110)\n",
    "annotate_bars(ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Metric Accuracy\n",
    "metric_accuracy = {}\n",
    "\n",
    "for ent, pred in zip(actual_cols, extracted_cols):\n",
    "    accuracy = (gt_vs_llm[ent] == gt_vs_llm[pred]).mean() * 100\n",
    "    metric_accuracy[ent] = accuracy\n",
    "\n",
    "accuracy_table = pd.DataFrame({\n",
    "    \"Entity\": actual_cols,\n",
    "    \"Accuracy (%)\": [metric_accuracy[e] for e in actual_cols]\n",
    "}).set_index(\"Entity\")\n",
    "\n",
    "print(\"=== Entity Extraction Accuracy Table ===\")\n",
    "print(accuracy_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for each metric\n",
    "for ent, pred in zip(actual_cols, extracted_cols):\n",
    "\n",
    "    # Define labels per entity\n",
    "    if ent in ['ER_STATUS_1','PR_STATUS_1']:\n",
    "        labels = ['positive','negative','not performed','blank']\n",
    "    elif ent in ['ER_SCORE_1','PR_SCORE_1']:\n",
    "        labels = ['0','2','3','4','5','6','7','8','blank']\n",
    "    else:\n",
    "        labels = ['negative (unknown)','negative (0)','negative (1+)','borderline (2+)','positive (3+)','not performed','blank']\n",
    "\n",
    "    # --- CONFUSION MATRIX ---\n",
    "    cm = confusion_matrix(gt_vs_llm[ent], gt_vs_llm[pred], labels=labels)\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    ax = sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        linewidths=0.1,\n",
    "        linecolor='grey'\n",
    "    )\n",
    "    plt.xlabel(\"Extracted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix for {ent}\")\n",
    "\n",
    "    # Hide zero text\n",
    "    for text in ax.texts:\n",
    "        if text.get_text() == \"0\":\n",
    "            text.set_text(\"\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        gt_vs_llm[ent],\n",
    "        gt_vs_llm[pred],\n",
    "        labels=labels,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    prf_table = pd.DataFrame({\n",
    "        \"Class\": labels,\n",
    "        \"Precision\": np.round(precision,4),\n",
    "        \"Recall\": np.round(recall,4),\n",
    "        \"F1 Score\": np.round(f1,4),\n",
    "        \"Support\": support\n",
    "    })\n",
    "\n",
    "    prf_table = prf_table.set_index(\"Class\")\n",
    "\n",
    "    print(f\"\\n=== Precision, Recall, F1 for {ent} ===\")\n",
    "    print(prf_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM vs Human accuracy comparison\n",
    "hum_cols = [f\"{c}_hum\" for c in actual_cols]\n",
    "\n",
    "human_accuracy = {}\n",
    "llm_accuracy = {}\n",
    "\n",
    "for act, ext, hum in zip(actual_cols, extracted_cols, hum_cols):\n",
    "    llm_accuracy[act] = (gt_vs_llm[act] == gt_vs_llm[ext]).mean() * 100\n",
    "    human_accuracy[act] = (gt_vs_hum[act] == gt_vs_hum[hum]).mean() * 100\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Entity\": actual_cols,\n",
    "    \"LLM Accuracy\": [llm_accuracy[e] for e in actual_cols],\n",
    "    \"Human Accuracy\": [human_accuracy[e] for e in actual_cols],\n",
    "})\n",
    "\n",
    "comparison_long = comparison_df.melt(id_vars=\"Entity\",\n",
    "                                     var_name=\"Source\",\n",
    "                                     value_name=\"Accuracy\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax4 = sns.barplot(data=comparison_long, x=\"Entity\", y=\"Accuracy\", hue=\"Source\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Human vs LLM Accuracy Comparison\")\n",
    "plt.ylim(0, 110)\n",
    "annotate_bars(ax4)\n",
    "ax4.legend(\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.02, 1),\n",
    "    borderaxespad=0,\n",
    "    frameon=True\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
