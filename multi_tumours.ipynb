{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Flag Multiple Tumours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from src.helpers import load_config_from_yaml\n",
    "from src.text_preprocessor import TextPreprocessor\n",
    "from src.prompt_builder import PromptBuilder\n",
    "from src.model_request import ModelRequest\n",
    "from src.extractor_pipeline import ExtractorPipeline\n",
    "from src.post_processor import PostProcessor\n",
    "from src.custom_logging import setup_logging\n",
    "from src.cme_evaluator import CMEEvaluator\n",
    "from src.load_data import save_eval_df_to_s3, load_dataframe_from_s3, save_dataframe_to_s3\n",
    "import config.pipeline_config as conf\n",
    "from config.validation_config import MultiSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define config file path\n",
    "conf_file_path = \"./config/local.yaml\"\n",
    "# Load config\n",
    "yaml_conf = load_config_from_yaml(file_path=conf_file_path)\n",
    "\n",
    "# Get bucket name and data name from config.\n",
    "bucket_name = yaml_conf.get(\"BUCKET_NAME\")\n",
    "data_name = yaml_conf.get(\"THE_DATA\")\n",
    "\n",
    "# Model and S3 info:\n",
    "model_id = yaml_conf.get(\"MODEL_ID\")\n",
    "model_args = yaml_conf.get(\"MODEL_ARGS\")\n",
    "output_folder = yaml_conf.get(\"MULTI_TUMOUR_S3_FOLDER\")\n",
    "cme_prompt_id = yaml_conf.get(\"MULTI_TUMOUR_PROMPT_MANAGEMENT_ID\")\n",
    "cme_prompt_name = yaml_conf.get(\"MULTI_TUMOUR_PROMPT_MANAGEMENT_NAME\")\n",
    "\n",
    "# Load in the record table. Use the .head(x) to only use the first x reports (useful for a quick test).\n",
    "float_columns = [f\"ER_SCORE_{i+1}\" for i in range(4)] + [f\"PR_SCORE_{i+1}\" for i in range(4)]\n",
    "records = load_dataframe_from_s3(bucket_name, data_name, float_columns).head(5)\n",
    "\n",
    "# Load dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(f\"There are {records.shape[0]} records in this dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processor = PostProcessor(records, conf.multi_tumour_accepted_values.keys(), conf.multi_tumour_accepted_values)\n",
    "records[\"Multiple Tumours\"] = records[\"Multiple Tumours\"].fillna(\"0\")\n",
    "records = records.apply(post_processor.apply_general_mapping, mapping = {\"y\":\"1\"}, cols_to_map = [\"Multiple Tumours\"], axis = 1)\n",
    "\n",
    "records[\"Multiple Tumours New\"] = records[\"Multiple Tumours New\"].fillna(\"0\")\n",
    "records = records.apply(post_processor.apply_general_mapping, mapping = {\"y\":\"1\"}, cols_to_map = [\"Multiple Tumours New\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Edit prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"SYSTEM PROMPT HERE\"\n",
    "prompt_layout = \"\"\"\n",
    "PROMPT HERE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE PIPELINE HERE\n",
    "setup_logging(enable_console=False,\n",
    "              enable_file=True,\n",
    "              console_log_level=conf.console_log_level,\n",
    "              log_dir=conf.log_dir)\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# prompter = PromptBuilder(model_id = model_id,\n",
    "#                          prompt_layout = prompt_layout,\n",
    "#                          system_prompt = system_prompt)\n",
    "\n",
    "# Use the below prompter instead if you want to define a prompt version from prompt management.\n",
    "prompter = PromptBuilder(model_id = model_id,\n",
    "                         system_prompt = conf.multi_tumour_system_prompt,\n",
    "                         prompt_id = cme_prompt_id,\n",
    "                         prompt_version = conf.multi_tumour_prompt_version)\n",
    "\n",
    "requester = ModelRequest(model_id,\n",
    "                         model_args,\n",
    "                         prompter)\n",
    "\n",
    "extractor_pipeline = ExtractorPipeline(config_file_path=conf_file_path,\n",
    "                                       preprocessor=preprocessor,\n",
    "                                       model_request=requester,\n",
    "                                       valid_structure=MultiSchema,\n",
    "                                       accepted_values = conf.multi_tumour_accepted_values)\n",
    "\n",
    "output_df = extractor_pipeline.run(df=records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE WHICH COLUMNS TO COMPARE\n",
    "original_compare_cols = {\"Multiple Tumours\": \"multi_tumour\",\"Multiple Tumours New\": \"multi_tumour\"}\n",
    "status_column = 'status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET UP THE EVALUATOR\n",
    "eval_df = records.merge(output_df, on = \"PATHOLOGY_ID\")\n",
    "\n",
    "evaluator = CMEEvaluator(comparison_dict=original_compare_cols,\n",
    "                         accepted_values=conf.multi_tumour_accepted_values,\n",
    "                         id_col=\"PATHOLOGY_ID\",\n",
    "                         df=eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator = CMEEvaluator(comparison_dict=original_compare_cols,\n",
    "#                          accepted_values=conf.multi_tumour_accepted_values,\n",
    "#                          id_col=\"PATHOLOGY_ID\",\n",
    "#                          bucket_name=bucket_name,\n",
    "#                          folder=output_folder,\n",
    "#                          list_saved=True\n",
    "#                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### First Check the breakdown of statuses\n",
    "\n",
    "* **valid**: This means output parsed to a JSON and all the keys of the JSON is present, and the values are in the accepted value list.\n",
    "* **partial**: This means the output parsed to a JSON, but some of the keys are missing or a value for a given key is not an accepted value.\n",
    "* **invalid**: This means the output parsed to a JSON, but none of the expected keys are present.\n",
    "* **validation_failed**: This means the output was unable to parse to JSON.\n",
    "\n",
    "We want to maximise the number of valids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.get_status_summary(status_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.get_validation_failed(status_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.get_invalid(status_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.get_non_accepted_summary_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.print_text(text_col=\"model_output\", id_val=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.plot_correctness_and_rowwise_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.plot_per_metric_plots_for_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Exploring the specific differences between Actual and Expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_column = \"Multiple Tumours New\"\n",
    "extracted_column = original_compare_cols[actual_column]\n",
    "\n",
    "# Change this to the value you see\n",
    "actual_value = \"0\" # i.e. positive\n",
    "extracted_value = \"1\" # i.e. negative\n",
    "\n",
    "# Extract out the values for eval_df\n",
    "evaluator.df[(evaluator.df[actual_column] == actual_value) & (evaluator.df[extracted_column] == extracted_value)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.print_text(text_col=\"preprocessed_REPORT\", id_val=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompter.list_prompt_versions(cme_prompt_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_description = \"DESCRIPTION\" # Tell me what was good about this run, was changes were made, etc. Why did you save it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves Prompt Version - you will need to make sure the version is the correct one. \n",
    "# prompter.save_prompt_version(cme_prompt_id, cme_prompt_name, 1, overall_description)\n",
    "\n",
    "# Saves Evaluation Outputs\n",
    "# save_eval_df_to_s3(df=eval_df,\n",
    "#                    bucket_name=bucket_name,\n",
    "#                    folder=output_folder,\n",
    "#                    description=overall_description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
