{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.helpers import load_config_from_yaml\n",
    "from src.text_preprocessor import TextPreprocessor\n",
    "from src.prompt_builder import PromptBuilder\n",
    "from src.model_request import ModelRequest\n",
    "from src.extractor_pipeline import ExtractorPipeline\n",
    "from src.post_processor import PostProcessor\n",
    "from src.custom_logging import setup_logging\n",
    "from src.cme_evaluator import CMEEvaluator\n",
    "from src.load_data import load_dataframe_from_s3, save_dataframe_to_s3\n",
    "import config.pipeline_config as conf\n",
    "from config.validation_config import ValidSchema, MultiSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_file_path = \"./config/local.yaml\"\n",
    "yaml_conf = load_config_from_yaml(file_path=conf_file_path)\n",
    "\n",
    "bucket_name = yaml_conf.get(\"BUCKET_NAME\")\n",
    "id_col = yaml_conf.get(\"ID_COL\")\n",
    "data_name = yaml_conf.get(\"THE_DATA\")\n",
    "\n",
    "model_id = yaml_conf.get(\"MODEL_ID\")\n",
    "model_args = yaml_conf.get(\"MODEL_ARGS\")\n",
    "output_folder = yaml_conf.get(\"YOUR_S3_FOLDER\")\n",
    "cme_prompt_id = yaml_conf.get(\"PROMPT_MANAGEMENT_ID\")\n",
    "cme_prompt_name = yaml_conf.get(\"PROMPT_MANAGEMENT_NAME\")\n",
    "\n",
    "cme_multi_prompt_id = yaml_conf.get(\"MULTI_TUMOUR_PROMPT_MANAGEMENT_ID\")\n",
    "cme_multi_prompt_name = yaml_conf.get(\"MULTI_TUMOUR_PROMPT_MANAGEMENT_NAME\")\n",
    "\n",
    "# Load in the records\n",
    "float_columns = [f\"ER_SCORE_{i+1}\" for i in range(4)] + [f\"PR_SCORE_{i+1}\" for i in range(4)]\n",
    "records = load_dataframe_from_s3(bucket_name, data_name, float_columns)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(f\"Loaded {records.shape[0]} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processor = PostProcessor(records, conf.multi_tumour_accepted_values.keys(), conf.multi_tumour_accepted_values)\n",
    "records[\"Multiple Tumours\"] = records[\"Multiple Tumours\"].fillna(\"0\")\n",
    "records = records.apply(post_processor.apply_general_mapping, mapping = {\"y\":\"1\"}, cols_to_map = [\"Multiple Tumours\"], axis = 1)\n",
    "\n",
    "records[\"Multiple Tumours New\"] = records[\"Multiple Tumours New\"].fillna(\"0\")\n",
    "records = records.apply(post_processor.apply_general_mapping, mapping = {\"y\":\"1\"}, cols_to_map = [\"Multiple Tumours New\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Flag Multiple Tumours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_logging(enable_console=False,\n",
    "              enable_file=True,\n",
    "              console_log_level=conf.console_log_level,\n",
    "              log_dir=conf.log_dir)\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "prompter = PromptBuilder(model_id = model_id,\n",
    "                         system_prompt = conf.multi_tumour_system_prompt,\n",
    "                         prompt_layout = conf.multi_tumour_prompt)\n",
    "\n",
    "requester = ModelRequest(model_id,\n",
    "                         model_args,\n",
    "                         prompter)\n",
    "\n",
    "extractor_pipeline = ExtractorPipeline(config_file_path=conf_file_path,\n",
    "                                       preprocessor=preprocessor,\n",
    "                                       model_request=requester,\n",
    "                                       valid_structure=MultiSchema,\n",
    "                                       accepted_values = conf.multi_tumour_accepted_values)\n",
    "\n",
    "multi_tumour_output_df = extractor_pipeline.run(df=records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_tumour_reports = multi_tumour_output_df[multi_tumour_output_df['multi_tumour']==\"1\"]\n",
    "single_tumour_reports = multi_tumour_output_df[multi_tumour_output_df['multi_tumour']==\"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(multi_tumour_reports)} reports were flagged as containing multiple tumours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Multi Tumour Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "There are two comparision columns for multiple tumours. 'Multiple Tumours' is based on the breast cancer registration guidelines definitions, 'Multiple Tumours New' is the same defintion but also includes reports that mention things like 'Tumour 1' and 'Tumour 2' and 'bilateral'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_compare_cols = {\"Multiple Tumours\": \"multi_tumour\",\"Multiple Tumours New\": \"multi_tumour\"}\n",
    "status_column = 'status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_eval_df = records.merge(multi_tumour_output_df, on = id_col)\n",
    "\n",
    "evaluator = CMEEvaluator(comparison_dict=multi_compare_cols,\n",
    "                         accepted_values=conf.multi_tumour_accepted_values,\n",
    "                         id_col=id_col,\n",
    "                         df=multi_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_bars(ax, fmt=\"{:.1f}%\"):\n",
    "    \"\"\"Annotate each bar in a barplot.\"\"\"\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        if height > 0:\n",
    "            ax.text(\n",
    "                x=p.get_x() + p.get_width() / 2,\n",
    "                y=height+1,\n",
    "                s=fmt.format(height),\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                color='black',\n",
    "                fontsize=10,\n",
    "                fontweight='bold'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.get_status_summary(status_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_cols = ['Multiple Tumours', 'Multiple Tumours New']\n",
    "extracted_cols = ['multi_tumour','multi_tumour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy per metric\n",
    "metric_accuracy = {}\n",
    "\n",
    "for ent, pred in zip(actual_cols, extracted_cols):\n",
    "    metric_accuracy[ent] = (evaluator.df[ent] == evaluator.df[pred]).mean() * 100\n",
    "\n",
    "acc_series = pd.Series(metric_accuracy)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax1 = sns.barplot(x=acc_series.index, y=acc_series.values)\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xlabel(\"Extracted Metrics\")\n",
    "plt.title(\"Accuracy per Entity\")\n",
    "plt.ylim(0, 110)\n",
    "annotate_bars(ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for each metric\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "for ent, pred in zip(actual_cols, extracted_cols):\n",
    "    labels = ['0','1']\n",
    "    cm = confusion_matrix(evaluator.df[ent], evaluator.df[pred], labels=labels)\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    ax = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=labels, yticklabels=labels, linewidths=0.1, linecolor='grey',)\n",
    "    plt.xlabel(\"Extracted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix for {ent}\")\n",
    "\n",
    "    for text in ax.texts:\n",
    "        if text.get_text() == \"0\":\n",
    "            text.set_text(\"\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Run Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "prompter = PromptBuilder(model_id = model_id,\n",
    "                         system_prompt = conf.extraction_system_prompt,\n",
    "                         prompt_layout = conf.gpt_oss_prompt,\n",
    "                         accepted_values = conf.accepted_values)\n",
    "\n",
    "requester = ModelRequest(model_id,\n",
    "                         model_args,\n",
    "                         prompter)\n",
    "\n",
    "extractor_pipeline = ExtractorPipeline(config_file_path=conf_file_path,\n",
    "                                       preprocessor=preprocessor,\n",
    "                                       model_request=requester,\n",
    "                                       valid_structure=ValidSchema,\n",
    "                                       accepted_values = conf.accepted_values)\n",
    "\n",
    "metrics_output_df = extractor_pipeline.run(df=single_tumour_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processor = PostProcessor(metrics_output_df, conf.accepted_values.keys(), conf.accepted_values)\n",
    "\n",
    "functions = {\n",
    "    \"map_two_part_scores\": post_processor.map_two_part_scores,\n",
    "    \"map_score\": post_processor.map_score,\n",
    "    \"score_to_status\": post_processor.score_to_status,\n",
    "    \"apply_general_mapping\": post_processor.apply_general_mapping,\n",
    "}\n",
    "\n",
    "settings = {\n",
    "    \"map_two_part_scores\": {\n",
    "        \"enabled\":True,\n",
    "        \"args\": [{\"cols_to_map\":[\"er_score\", \"pr_score\"]}]\n",
    "    },\n",
    "    \"map_score\": {\n",
    "        \"enabled\": True,\n",
    "        \"args\": [{\"cols_to_map\":[\"er_score\", \"pr_score\"]}]\n",
    "    },\n",
    "    \"score_to_status\": {\n",
    "        \"enabled\": True,\n",
    "        \"args\": [{\"pairs\": [(\"er_score\",\"er_status\"),(\"pr_score\",\"pr_status\")]}]\n",
    "    },\n",
    "    \"apply_general_mapping\": {\n",
    "        \"enabled\": True,\n",
    "        \"args\": [\n",
    "            {\"mapping\":{\"0\": \"negative (0)\", \"1+\": \"negative (1+)\", \"2+\": \"borderline (2+)\", \"3+\": \"positive (3+)\"},\n",
    "                 \"cols_to_map\":[\"her2_status\"]},\n",
    "            {\"mapping\":{\"null\": np.nan},\n",
    "                 \"cols_to_map\":[\"er_status\", \"er_score\", \"pr_status\", \"pr_score\", \"her2_status\"]}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "metrics_output_df_processed = post_processor.run(functions, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Results for Metric Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_compare_cols = {\"ER_STATUS_1\": \"er_status_p\",\n",
    "                         \"ER_SCORE_1\": \"er_score_p\",\n",
    "                         \"PR_STATUS_1\": \"pr_status_p\",\n",
    "                         \"PR_SCORE_1\": \"pr_score_p\",\n",
    "                         \"HER2_STATUS_1\": \"her2_status_p\"}\n",
    "status_column = 'status_processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_eval_df = records.merge(metrics_output_df_processed, on = id_col)\n",
    "\n",
    "evaluator = CMEEvaluator(comparison_dict=metric_compare_cols,\n",
    "                         accepted_values=conf.final_accepted_values,\n",
    "                         id_col=id_col,\n",
    "                         df=metric_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.get_status_summary(status_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.get_non_accepted_summary_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_cols_1 = ['ER_SCORE_1','PR_SCORE_1']\n",
    "results_df = evaluator.df.copy()\n",
    "ground_truth_data = load_dataframe_from_s3(bucket_name,data_name, float_cols_1)\n",
    "\n",
    "actual_cols = ['ER_STATUS_1','ER_SCORE_1','PR_STATUS_1','PR_SCORE_1','HER2_STATUS_1']\n",
    "extracted_cols = ['er_status_p','er_score_p','pr_status_p','pr_score_p','her2_status_p']\n",
    "llm_extraction = results_df[[id_col]+extracted_cols].fillna('blank')\n",
    "ground_truth_data = ground_truth_data[[id_col]+actual_cols].fillna('blank')\n",
    "\n",
    "gt_vs_llm = ground_truth_data.merge(llm_extraction, on = id_col, how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-document number of correct predictions\n",
    "gt_vs_llm[\"num_correct\"] = (gt_vs_llm[actual_cols].values == gt_vs_llm[extracted_cols].values).sum(axis=1)\n",
    "\n",
    "correct_dist = gt_vs_llm[\"num_correct\"].value_counts(normalize=True).reindex([0,1,2,3,4,5]).sort_index() * 100\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax1 = sns.barplot(x=['0','1','2','3','4','5'], y=correct_dist.values)\n",
    "plt.xlabel(\"Number of correctly extracted entities\")\n",
    "plt.ylabel(\"Percentage of documents (%)\")\n",
    "plt.title(\"Distribution of Documents by Number of Correct Entity Extractions\")\n",
    "plt.ylim(0, 110)\n",
    "annotate_bars(ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy per metric\n",
    "metric_accuracy = {}\n",
    "\n",
    "for ent, pred in zip(actual_cols, extracted_cols):\n",
    "    metric_accuracy[ent] = (gt_vs_llm[ent] == gt_vs_llm[pred]).mean() * 100\n",
    "\n",
    "acc_series = pd.Series(metric_accuracy)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax1 = sns.barplot(x=acc_series.index, y=acc_series.values)\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xlabel(\"Extracted Metrics\")\n",
    "plt.title(\"Accuracy per Entity\")\n",
    "plt.ylim(0, 110)\n",
    "annotate_bars(ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for each metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "for ent, pred in zip(actual_cols, extracted_cols):\n",
    "    if ent in ['ER_STATUS_1','PR_STATUS_1']:\n",
    "        labels = ['positive','negative','not performed','blank']\n",
    "    elif ent in ['ER_SCORE_1','PR_SCORE_1']:\n",
    "        labels = ['0','2','3','4','5','6','7','8','blank']\n",
    "    else:\n",
    "        labels = ['negative (unknown)','negative (0)','negative (1+)','borderline (2+)','positive (3+)','not performed','blank']\n",
    "    cm = confusion_matrix(gt_vs_llm[ent], gt_vs_llm[pred], labels=labels)\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    ax = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=labels, yticklabels=labels, linewidths=0.1, linecolor='grey',)\n",
    "    plt.xlabel(\"Extracted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix for {ent}\")\n",
    "\n",
    "    for text in ax.texts:\n",
    "        if text.get_text() == \"0\":\n",
    "            text.set_text(\"\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = pd.concat([metrics_output_df_processed, multi_tumour_reports], join='outer', ignore_index=True)\n",
    "all_eval_df = multi_tumour_output_df.drop([\"REPORT\", \"preprocessed_REPORT\"], axis = 1).merge(metrics_output_df_processed.drop([\"REPORT\", \"preprocessed_REPORT\"], axis = 1), on = id_col, how='left', suffixes=('_multi',None))\n",
    "all_eval_df = records.merge(all_eval_df, on = id_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to S3 if required\n",
    "# file_name = \"descriptive file name here\"\n",
    "# save_dataframe_to_s3(all_eval_df,bucket_name,f\"final_outputs/{file_name}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
