{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Prompt Engineering WorkFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from src.helpers import load_config_from_yaml\n",
    "from src.text_preprocessor import TextPreprocessor\n",
    "from src.prompt_builder import PromptBuilder\n",
    "from src.model_request import ModelRequest\n",
    "from src.extractor_pipeline import ExtractorPipeline\n",
    "from src.post_processor import PostProcessor\n",
    "from src.custom_logging import setup_logging\n",
    "from src.cme_evaluator import CMEEvaluator\n",
    "from src.load_data import save_eval_df_to_s3, load_dataframe_from_s3, save_dataframe_to_s3\n",
    "import config.pipeline_config as conf\n",
    "from config.validation_config import ValidSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Load and Initiate Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define config file path\n",
    "conf_file_path = \"./config/local.yaml\"\n",
    "# Load config\n",
    "yaml_conf = load_config_from_yaml(file_path=conf_file_path)\n",
    "\n",
    "# Get bucket name and data name from config.\n",
    "bucket_name = yaml_conf.get(\"BUCKET_NAME\")\n",
    "data_name = yaml_conf.get(\"THE_DATA\")\n",
    "\n",
    "# Model and S3 info:\n",
    "model_id = yaml_conf.get(\"MODEL_ID\")\n",
    "model_args = yaml_conf.get(\"MODEL_ARGS\")\n",
    "output_folder = yaml_conf.get(\"YOUR_S3_FOLDER\")\n",
    "cme_prompt_id = yaml_conf.get(\"PROMPT_MANAGEMENT_ID\")\n",
    "cme_prompt_name = yaml_conf.get(\"PROMPT_MANAGEMENT_NAME\")\n",
    "\n",
    "# Load in the record table. Use the .head(x) to only use the first x reports (useful for a quick test).\n",
    "float_columns = [f\"ER_SCORE_{i+1}\" for i in range(4)] + [f\"PR_SCORE_{i+1}\" for i in range(4)]\n",
    "records = load_dataframe_from_s3(bucket_name, data_name, float_columns).head(10)\n",
    "\n",
    "# To keep things simple for now we are only looking at records where one tumour is present\n",
    "records = records[records['Multiple Tumours'] != 'Y']\n",
    "\n",
    "# Load dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(f\"There are {records.shape[0]} records in this dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Editing the Prompt\n",
    "This is where you edit the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EDIT YOUR PROMPT HERE\n",
    "system_prompt = None# Llama and Claude models only\n",
    "prompt_layout = \"\"\"\n",
    "You have this document:\n",
    "{document}\n",
    "\n",
    "I would like you to extract out only ER Status, ER Score, PR Status, PR Score \n",
    "and HER2 Status and return the output in a JSON markdown structure. Exclude explanations and extra information from your response.\n",
    "\n",
    "Every entity extracted must have a value from the accepted values below:\n",
    "\n",
    "{accepted_values}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Running the Extractor Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "This can take up to 10 minutes when running it on all reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE PIPELINE HERE\n",
    "setup_logging(enable_console=False,\n",
    "              enable_file=True,\n",
    "              console_log_level=conf.console_log_level,\n",
    "              log_dir=conf.log_dir)\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "prompter = PromptBuilder(model_id = model_id,\n",
    "                         prompt_layout = prompt_layout,\n",
    "                         system_prompt = system_prompt,\n",
    "                         accepted_values = conf.accepted_values)\n",
    "\n",
    "# Use the below prompter instead if you want to define a prompt version from prompt management.\n",
    "# prompter = PromptBuilder(model_id = model_id,\n",
    "#                          system_prompt = system_prompt,\n",
    "#                          prompt_id = cme_prompt_id,\n",
    "#                          prompt_version = 0,\n",
    "#                          accepted_values = conf.accepted_values)\n",
    "\n",
    "requester = ModelRequest(model_id,\n",
    "                         model_args,\n",
    "                         prompter)\n",
    "\n",
    "extractor_pipeline = ExtractorPipeline(config_file_path=conf_file_path,\n",
    "                                       preprocessor=preprocessor,\n",
    "                                       model_request=requester,\n",
    "                                       valid_structure=ValidSchema,\n",
    "                                       accepted_values = conf.accepted_values)\n",
    "\n",
    "output_df = extractor_pipeline.run(df=records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "If you would like to reload a prompt, this will list the prompt versions with their respective descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIST VERSIONS OF YOUR PROMPT\n",
    "prompter.list_prompt_versions(cme_prompt_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Post Process\n",
    "Applies the post processing steps to the LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processor = PostProcessor(output_df, conf.accepted_values.keys())\n",
    "\n",
    "functions = {\n",
    "    \"map_two_part_scores\": post_processor.map_two_part_scores,\n",
    "    \"map_score\": post_processor.map_score,\n",
    "    \"score_to_status\": post_processor.score_to_status,\n",
    "    \"apply_general_mapping\": post_processor.apply_general_mapping,\n",
    "}\n",
    "\n",
    "# Configure which post-processing function to run, which columns they run on and their mapping instructions\n",
    "settings = {\n",
    "    \"map_two_part_scores\": {\n",
    "        \"enabled\":True,\n",
    "        \"args\": [{\"cols_to_map\":[\"er_score\", \"pr_score\"]}]\n",
    "    },\n",
    "    \"map_score\": {\n",
    "        \"enabled\": True,\n",
    "        \"args\": [{\"cols_to_map\":[\"er_score\", \"pr_score\"]}]\n",
    "    },\n",
    "    \"score_to_status\": {\n",
    "        \"enabled\": True,\n",
    "        \"args\": [{\"pairs\": [(\"er_score\",\"er_status\"),(\"pr_score\",\"pr_status\")]}]\n",
    "    },\n",
    "    \"apply_general_mapping\": {\n",
    "        \"enabled\": True,\n",
    "        \"args\": [\n",
    "            {\"mapping\":{\"0\": \"negative (0)\", \"1+\": \"negative (1+)\", \"2+\": \"borderline (2+)\", \"3+\": \"positive (3+)\"},\n",
    "                 \"cols_to_map\":[\"her2_status\"]},\n",
    "            {\"mapping\":{\"null\": np.nan},\n",
    "                 \"cols_to_map\":[\"er_status\", \"er_score\", \"pr_status\", \"pr_score\", \"her2_status\"]}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "output_df_processed = post_processor.run(functions, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Set-up Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "This creates the comparison dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE WHICH COLUMNS TO COMPARE\n",
    "original_compare_cols = {\"ER_STATUS_1\": \"er_status_p\",\n",
    "                         \"ER_SCORE_1\": \"er_score_p\",\n",
    "                         \"PR_STATUS_1\": \"pr_status_p\",\n",
    "                         \"PR_SCORE_1\": \"pr_score_p\",\n",
    "                         \"HER2_STATUS_1\": \"her2_status_p\"}\n",
    "status_column = 'status_processed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "This initiates the evaluator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET UP THE EVALUATOR\n",
    "eval_df = records.merge(output_df_processed, on = \"PATHOLOGY_ID\")\n",
    "\n",
    "evaluator = CMEEvaluator(comparison_dict=original_compare_cols,\n",
    "                         accepted_values=conf.final_accepted_values,\n",
    "                         id_col=\"PATHOLOGY_ID\",\n",
    "                         df=eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT THIS - if you want to load the results for a specific description. i.e. that compliements your reloaded prompt_version.\n",
    "\n",
    "# evaluator = CMEEvaluator(comparison_dict=original_compare_cols,\n",
    "#                          accepted_values=conf.final_accepted_values,\n",
    "#                          id_col=\"PATHOLOGY_ID\",\n",
    "#                          bucket_name=bucket_name,\n",
    "#                          folder=output_folder,\n",
    "#                          list_saved=True\n",
    "#                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Evaluating the JSON Output and Unaccepted Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### First Check the breakdown of statuses\n",
    "\n",
    "* **valid**: This means output parsed to a JSON and all the keys of the JSON is present, and the values are in the accepted value list.\n",
    "* **partial**: This means the output parsed to a JSON, but some of the keys are missing or a value for a given key is not an accepted value.\n",
    "* **invalid**: This means the output parsed to a JSON, but none of the expected keys are present.\n",
    "* **validation_failed**: This means the output was unable to parse to JSON.\n",
    "\n",
    "We want to maximise the number of valids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.get_status_summary(status_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Note down why some were \"validation failed\"\n",
    "JSON has to be of a particular structure. So it might be there are special characters in the JSON or extra commas when there shouldn't be etc.\n",
    "\n",
    "Any notes here can be fed into the prompt, but might also need to be changed in the post-processing of the JSON itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "pd.set_option(\"display.max_rows\", 300)\n",
    "evaluator.get_validation_failed(status_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Note down why some were \"invalid\"\n",
    "There might be something wrong with how the JSON is being constructed with the keys, such as weird characters in the keys, or completely wrong value of the key.\n",
    "\n",
    "Any notes here can be fed into the prompt, but might also need to be changed in the post-processing of the JSON itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "pd.set_option(\"display.max_rows\", 300)\n",
    "evaluator.get_invalid(status_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Note down why some were \"partially invalid\".\n",
    "In this scenario some of the JSON's keys might be wrongly defined. \n",
    "\n",
    "Any notes here can be fed into the prompt, but might also need to be changed in the post-processing of the JSON itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "pd.set_option(\"display.max_rows\", 300)\n",
    "evaluator.get_partial(status_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Note down when a non-accepted value has been extracted out instead\n",
    "\n",
    "Below is the list of accepted values for each Status and Score. \n",
    "\n",
    "The LLM might have extracted out something different. Use this as an opportunity to work out how to improve the instructions fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.accepted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will print ALL of the non-accepted values. If this is too long then comment this out.\n",
    "# And uncomment the code below for specific \"actual_cols\"\n",
    "evaluator.get_non_accepted_summary_all()\n",
    "\n",
    "# If you want to look at a specific \"actual_col\" you can use the commented function below\n",
    "#evaluator.get_non_accepted_summary(\"HER2_STATUS_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "If you are unsure as to why a value has been extracted like this, you can print reports below to show values for a given text id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.print_text(text_col=\"REPORT_x\", id_val=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Evaluating the Correctness of Extraction\n",
    "\n",
    "The plot below will give an overview of:\n",
    "1. **Percentage of Correct Metric Count Across Reports**: This will be for each row the number of values it has got correct out of ER/PR/HER2, status and score.\n",
    "2. **Percentage of Correct per Comparison Column**: For each of ER/PR/HER2 status/score, how much of it was extracted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.plot_correctness_and_rowwise_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "The plot below will give an overview of for each ER/PR/HER2 status/score:\n",
    "1. **Actual V Extracted Matrix**: This is a count matrix, to show what the actual value is in the report, and what was actually extracted.\n",
    "   * **key_missing**: This means a key was missing from the JSON for this value.\n",
    "   * **validation_failed**: This wasn't extracted because the validation failed.\n",
    "   * **non-accepted values**: Highlights if any were assigned to a value that is not in the accepted list.\n",
    "2. **Per-Value Metrics**: This displays the precision, recall, and f1 score for each value that is extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluator.plot_per_metric_plots_for_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## Exploring the specific differences between Actual and Expected\n",
    "\n",
    "The cell below will retrieve the documents where the \"wrong\" value has been extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "actual_column = \"PR_SCORE_1\" # ER_STATUS_1, PR_STATUS_1, ER_SCORE_1, PR_SCORE_1, HER2_STATUS_1\n",
    "extracted_column = original_compare_cols[actual_column]\n",
    "\n",
    "# Change this to the value you see\n",
    "actual_value = \"1\" # i.e. positive\n",
    "extracted_value = \"0\" # i.e. negative\n",
    "\n",
    "# Extract out the values for eval_df\n",
    "evaluator.df[(evaluator.df[actual_column] == actual_value) & (evaluator.df[extracted_column] == extracted_value)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Use the cell below to explore the reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.print_text(text_col=\"REPORT_x\", id_val=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Lastly! Was this a good run? Do you want to save something about this run? \n",
    "\n",
    "If so please run the last two cells to save your prompt and eval dataframe. We will use the same description here so we can link up prompt with outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what version to write, please use this cell to help display the last version saved.\n",
    "prompter.list_prompt_versions(cme_prompt_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_description = \"EDIT ME\" # Tell me what was good about this run, was changes were made, etc. Why did you save it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves Prompt Version - you will need to make sure the version is the correct one. \n",
    "prompter.save_prompt_version(cme_prompt_id, cme_prompt_name, 1, overall_description)\n",
    "\n",
    "# Saves Evaluation Outputs\n",
    "save_eval_df_to_s3(df=eval_df,\n",
    "                   bucket_name=bucket_name,\n",
    "                   folder=output_folder,\n",
    "                   description=overall_description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
